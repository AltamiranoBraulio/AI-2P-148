# ---------------------------------------------------
# üìö Importamos librer√≠as necesarias

import numpy as np  # Importamos NumPy, que es una librer√≠a para operaciones matem√°ticas y matrices (arreglos)
import matplotlib.pyplot as plt  # Importamos Matplotlib para poder graficar nuestros resultados de forma visual

# ---------------------------------------------------
# üî¢ Definimos funciones matem√°ticas llamadas "Funciones de Activaci√≥n"
# Son usadas en redes neuronales para decidir si una neurona "se activa" o no.

# üìà Funci√≥n Sigmoide
def sigmoid(x):  # Definimos una funci√≥n llamada sigmoid que toma una variable 'x'
    return 1 / (1 + np.exp(-x))  # Devuelve el resultado de la f√≥rmula 1 / (1 + e^(-x)), que siempre da valores entre 0 y 1

# üü© Funci√≥n ReLU (Rectified Linear Unit)
def relu(x):  # Definimos la funci√≥n ReLU que toma 'x'
    return np.maximum(0, x)  # Devuelve 'x' si es positivo y 0 si es negativo (es decir, filtra todos los valores negativos)

# üîÑ Funci√≥n Tangente Hiperb√≥lica (Tanh)
def tanh(x):  # Definimos la funci√≥n tanh que toma 'x'
    return np.tanh(x)  # Devuelve el valor tanh de 'x', que siempre estar√° entre -1 y 1 (m√°s fuerte que sigmoide)

# üéØ Funci√≥n Softmax (convierte un vector en probabilidades)
def softmax(x):  # Definimos la funci√≥n softmax que toma 'x' (puede ser un array)
    e_x = np.exp(x - np.max(x))  # Calculamos e^x para cada elemento pero primero le restamos el m√°ximo para estabilizaci√≥n num√©rica
    return e_x / e_x.sum(axis=0)  # Dividimos cada e^x por la suma total, as√≠ obtenemos "probabilidades" que suman 1

# ---------------------------------------------------
# üé® Graficamos las funciones para entenderlas visualmente

# Creamos un array llamado 'x' que contiene 100 valores entre -10 y 10 (espaciados de forma uniforme)
x = np.linspace(-10, 10, 100)

# Creamos una figura donde vamos a poner 4 gr√°ficos. Ajustamos su tama√±o a 12x8 pulgadas.
plt.figure(figsize=(12, 8))

# ---------------------------------------------------
# üîµ Graficamos la funci√≥n Sigmoide

plt.subplot(2, 2, 1)  # Dividimos la ventana en 2 filas y 2 columnas y usamos el primer espacio
plt.plot(x, sigmoid(x), color='blue')  # Graficamos los valores de x contra su valor sigmoide, en color azul
plt.title('Sigmoide')  # Ponemos t√≠tulo al gr√°fico
plt.grid(True)  # Activamos la grilla para mejor visualizaci√≥n

# ---------------------------------------------------
# üü© Graficamos la funci√≥n ReLU

plt.subplot(2, 2, 2)  # Usamos el segundo espacio en la ventana (arriba a la derecha)
plt.plot(x, relu(x), color='green')  # Graficamos x contra ReLU(x), en color verde
plt.title('ReLU')  # T√≠tulo del gr√°fico
plt.grid(True)  # Activamos grilla

# ---------------------------------------------------
# üü£ Graficamos la funci√≥n Tanh

plt.subplot(2, 2, 3)  # Tercer espacio (abajo a la izquierda)
plt.plot(x, tanh(x), color='purple')  # Graficamos x contra tanh(x), en color p√∫rpura
plt.title('Tanh')  # T√≠tulo del gr√°fico
plt.grid(True)  # Activamos grilla

# ---------------------------------------------------
# üü† Graficamos la funci√≥n Softmax

plt.subplot(2, 2, 4)  # Cuarto espacio (abajo a la derecha)

# Creamos un peque√±o vector [1, 2, 3] y lo pasamos a la funci√≥n softmax para obtener sus probabilidades
vector_softmax = softmax(np.array([x1 for x1 in [1, 2, 3]]))  

# Graficamos un gr√°fico de barras para visualizar la salida softmax
plt.bar([1, 2, 3], vector_softmax, color='orange')  # Dibujamos 3 barras con los valores resultantes y color naranja
plt.title('Softmax (vector)')  # T√≠tulo del gr√°fico
plt.grid(True)  # Activamos grilla

# ---------------------------------------------------
# üìè Ajustamos la distribuci√≥n de los gr√°ficos para que no se encimen
plt.tight_layout()

# üëÅÔ∏è Mostramos la ventana con todos los gr√°ficos
plt.show()
