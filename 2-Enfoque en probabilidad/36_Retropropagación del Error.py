# ğŸ“š Importamos librerÃ­as necesarias
import numpy as np  # Para operaciones numÃ©ricas
import matplotlib.pyplot as plt  # Para graficar
from sklearn.datasets import make_moons  # Para generar datos de juguete no lineales
from sklearn.model_selection import train_test_split  # Para separar datos en entrenamiento y prueba
from sklearn.preprocessing import OneHotEncoder  # Para convertir etiquetas en formato one-hot

# ----------------------------------------------------
# ğŸ² Generamos un conjunto de datos (dos lunas entrelazadas)
X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)  # Generamos 1000 puntos con ruido

# ğŸ¯ Convertimos las etiquetas en formato one-hot (ej: clase 0 â” [1, 0], clase 1 â” [0, 1])
encoder = OneHotEncoder(sparse_output=False)  # âœ… Usamos sparse_output=False para versiones nuevas de scikit-learn
y_onehot = encoder.fit_transform(y.reshape(-1, 1))  # Convertimos y a matriz one-hot

# ğŸ”€ Separamos en datos de entrenamiento (80%) y prueba (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)

# ----------------------------------------------------
# ğŸ§  Clase que implementa una Red Neuronal Multicapa simple con retropropagaciÃ³n
class SimpleNeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):
        # Inicializamos los pesos de entrada a capa oculta (valores pequeÃ±os aleatorios)
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        # Inicializamos los pesos de capa oculta a capa de salida
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.lr = learning_rate  # Tasa de aprendizaje

    def sigmoid(self, x):
        # ğŸŸ¢ FunciÃ³n de activaciÃ³n sigmoide (suaviza valores entre 0 y 1)
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        # ğŸŸ¢ Derivada de la funciÃ³n sigmoide (usada en retropropagaciÃ³n)
        return x * (1 - x)

    def fit(self, X, y, epochs=1000):
        for epoch in range(epochs):
            # ğŸš€ FORWARD PASS (propagaciÃ³n hacia adelante)
            z1 = np.dot(X, self.W1)  # Entrada a capa oculta
            a1 = self.sigmoid(z1)  # ActivaciÃ³n de capa oculta

            z2 = np.dot(a1, self.W2)  # Entrada a capa de salida
            a2 = self.sigmoid(z2)  # ActivaciÃ³n (salida final)

            # ğŸ¯ Calculamos el error (diferencia entre salida deseada y salida actual)
            error = y - a2

            # ğŸ” BACKPROPAGATION (retropropagaciÃ³n del error)
            d2 = error * self.sigmoid_derivative(a2)  # Gradiente para capa salida
            d1 = np.dot(d2, self.W2.T) * self.sigmoid_derivative(a1)  # Gradiente para capa oculta

            # ğŸ”„ Ajustamos pesos usando los gradientes
            self.W2 += self.lr * np.dot(a1.T, d2)
            self.W1 += self.lr * np.dot(X.T, d1)

            # ğŸ“¢ Imprimimos el error cada 100 epochs
            if epoch % 100 == 0:
                loss = np.mean(np.abs(error))  # Error promedio
                print(f"Epoch {epoch}, Error: {loss:.4f}")

    def predict(self, X):
        # ğŸš€ Paso hacia adelante para predicciones
        z1 = np.dot(X, self.W1)
        a1 = self.sigmoid(z1)
        z2 = np.dot(a1, self.W2)
        a2 = self.sigmoid(z2)
        return np.argmax(a2, axis=1)  # Elegimos la clase con mayor probabilidad

# ----------------------------------------------------
# ğŸš€ Creamos y entrenamos la red neuronal
nn = SimpleNeuralNetwork(input_size=2, hidden_size=5, output_size=2, learning_rate=0.1)
nn.fit(X_train, y_train, epochs=1000)

# ----------------------------------------------------
# ğŸ” Evaluamos la red en los datos de prueba
y_pred = nn.predict(X_test)  # Predicciones
y_true = np.argmax(y_test, axis=1)  # Etiquetas verdaderas

# ğŸ¯ Calculamos la precisiÃ³n
accuracy = np.mean(y_pred == y_true)
print(f"\nğŸ” PrecisiÃ³n en datos de prueba: {accuracy * 100:.2f}%")

# ----------------------------------------------------
# ğŸ¨ Graficamos la frontera de decisiÃ³n aprendida por la red

# Creamos una malla de puntos para graficar
xx, yy = np.meshgrid(np.linspace(-2, 3, 100), np.linspace(-1.5, 2, 100))
grid = np.c_[xx.ravel(), yy.ravel()]

# Predicciones para cada punto en la malla
Z = nn.predict(grid)
Z = Z.reshape(xx.shape)

# ğŸ”µ Dibujamos la frontera
plt.contourf(xx, yy, Z, alpha=0.5, cmap='coolwarm')

# ğŸ”´ Dibujamos los puntos reales
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolor='k')

plt.title('ğŸ”€ Frontera de decisiÃ³n aprendida por la Red Neuronal')
plt.xlabel('x1')
plt.ylabel('x2')
plt.show()
