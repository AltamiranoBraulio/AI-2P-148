#  Importamos librer铆as necesarias
import numpy as np  # Librer铆a para operaciones matem谩ticas, especialmente matrices y vectores
import matplotlib.pyplot as plt  # Librer铆a para crear gr谩ficos y visualizaciones

# ---------------------------------------------------
#  Definimos la clase Perceptron (modelo cl谩sico de red neuronal)
class Perceptron:
    # M茅todo para inicializar el objeto Perceptron
    def __init__(self, input_size, learning_rate=0.1):
        # Creamos un vector de pesos inicializados en cero. 
        # input_size + 1 es porque agregamos un peso adicional para el sesgo (bias).
        self.weights = np.zeros(input_size + 1)
        # Guardamos la tasa de aprendizaje que nos dice qu茅 tan grandes son los ajustes en los pesos
        self.lr = learning_rate

    # Definimos la funci贸n de activaci贸n (funci贸n escal贸n)
    def activation_fn(self, x):
        # Si x es mayor o igual a 0, devuelve 1; si no, devuelve 0.
        # Esta funci贸n decide si la neurona "se activa" o no.
        return np.where(x >= 0, 1, 0)

    # M茅todo para hacer una predicci贸n con el perceptr贸n
    def predict(self, x):
        # Calculamos la suma ponderada (producto punto entre pesos y entradas)
        z = self.weights.T.dot(x)
        # Aplicamos la funci贸n de activaci贸n para obtener la salida final (0 o 1)
        return self.activation_fn(z)

    # M茅todo para entrenar el perceptr贸n
    def fit(self, X, d, epochs=10):
        # Repetimos el proceso de entrenamiento varias veces (epochs)
        for _ in range(epochs):
            # Iteramos sobre cada par de muestra (xi) y su etiqueta deseada (target)
            for xi, target in zip(X, d):
                # Calculamos la predicci贸n actual
                output = self.predict(xi)
                # Calculamos el error (deseado - predicho)
                error = target - output
                # Ajustamos los pesos seg煤n el error. 
                # La f贸rmula: nuevo_peso = peso_actual + tasa_aprendizaje * error * entrada
                self.weights += self.lr * error * xi

# ---------------------------------------------------
#  Definimos la clase ADALINE (versi贸n mejorada del Perceptr贸n)
class Adaline:
    # M茅todo para inicializar el objeto Adaline
    def __init__(self, input_size, learning_rate=0.01):
        # Pesos iniciales en cero (incluye el peso para el sesgo)
        self.weights = np.zeros(input_size + 1)
        # Tasa de aprendizaje
        self.lr = learning_rate

    # Calcula la suma ponderada de entradas y pesos (sin funci贸n de activaci贸n)
    def net_input(self, x):
        return self.weights.T.dot(x)

    # Funci贸n de activaci贸n (aqu铆 es similar al perceptr贸n, pero se aplica despu茅s del ajuste)
    def activation_fn(self, x):
        # Si la suma es mayor o igual a 0.5, devuelve 1; si no, devuelve 0
        return np.where(x >= 0.5, 1, 0)

    # M茅todo para predecir usando ADALINE
    def predict(self, x):
        # Calculamos la suma ponderada
        z = self.net_input(x)
        # Aplicamos la funci贸n de activaci贸n para obtener 0 o 1
        return self.activation_fn(z)

    # M茅todo para entrenar ADALINE
    def fit(self, X, d, epochs=10):
        # Repetimos el entrenamiento varias veces
        for _ in range(epochs):
            # Iteramos sobre cada muestra y su salida deseada
            for xi, target in zip(X, d):
                # Calculamos la salida (sin activar)
                output = self.net_input(xi)
                # Calculamos el error
                error = target - output
                # Ajustamos los pesos usando la Regla Delta
                self.weights += self.lr * error * xi

# ---------------------------------------------------
#  Definimos los datos de entrada para entrenar (caso: funci贸n OR l贸gica)
X = np.array([
    [1, 0, 0],  # Entrada 1: sesgo=1, x1=0, x2=0
    [1, 0, 1],  # Entrada 2: sesgo=1, x1=0, x2=1
    [1, 1, 0],  # Entrada 3: sesgo=1, x1=1, x2=0
    [1, 1, 1]   # Entrada 4: sesgo=1, x1=1, x2=1
])

#  Salidas deseadas (target) para la funci贸n OR
# OR(0,0)=0, OR(0,1)=1, OR(1,0)=1, OR(1,1)=1
d = np.array([0, 1, 1, 1])

# ---------------------------------------------------
#  Entrenamos y probamos el Perceptr贸n

# Creamos una instancia del Perceptr贸n con 2 entradas (x1 y x2)
p = Perceptron(input_size=2)
# Llamamos al m茅todo fit para entrenarlo con los datos X y las salidas d
p.fit(X, d)

# Imprimimos los resultados del Perceptr贸n despu茅s del entrenamiento
print(" Perceptr贸n resultados:")
# Iteramos sobre cada entrada para hacer predicciones
for x in X:
    # Imprimimos las entradas (x1 y x2) y la predicci贸n que hace el modelo
    print(f"Entrada: {x[1:]}, Predicci贸n: {p.predict(x)}")

# ---------------------------------------------------
#  Entrenamos y probamos ADALINE

# Creamos una instancia de ADALINE con 2 entradas
a = Adaline(input_size=2)
# Entrenamos el ADALINE con los datos
a.fit(X, d)

# Imprimimos los resultados de ADALINE despu茅s del entrenamiento
print("\n ADALINE resultados:")
# Iteramos sobre cada entrada para hacer predicciones
for x in X:
    # Imprimimos las entradas y la predicci贸n hecha por ADALINE
    print(f"Entrada: {x[1:]}, Predicci贸n: {a.predict(x)}")

# ---------------------------------------------------
#  Graficamos los puntos de entrada en 2D para visualizaci贸n

# Usamos scatter plot para graficar los puntos:
# Eje X: valores de x1 (0,0,1,1)
# Eje Y: valores de x2 (0,1,0,1)
# Colores: usamos 'd' para colorear seg煤n la clase (rojo=0, azul=1)
plt.scatter([0, 0, 1, 1], [0, 1, 0, 1], c=d, cmap='bwr', s=100)  # Tama帽o 100 para que se vean grandes
plt.title('Puntos clasificados (rojo=0, azul=1)')  # T铆tulo del gr谩fico
plt.grid(True)  # Activamos la cuadr铆cula
plt.show()  # Mostramos el gr谩fico
