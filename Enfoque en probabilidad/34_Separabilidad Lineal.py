# ğŸ“š Importamos las librerÃ­as necesarias
import numpy as np  # Para operaciones numÃ©ricas y generaciÃ³n de datos
import matplotlib.pyplot as plt  # Para graficar datos y resultados

# ---------------------------------------------------
# ğŸ§  Definimos la clase PerceptrÃ³n (nuestro modelo de aprendizaje)
class Perceptron:
    # MÃ©todo constructor: inicializa el perceptrÃ³n
    def __init__(self, input_size, learning_rate=0.1):
        # Creamos un vector de pesos inicializado en ceros (incluye bias, por eso +1)
        self.weights = np.zeros(input_size + 1)  # Ejemplo: si input_size=2, tendremos 3 pesos (w0, w1, w2)
        self.lr = learning_rate  # Tasa de aprendizaje (quÃ© tan rÃ¡pido aprende el perceptrÃ³n)

    # ğŸ”¥ FunciÃ³n de activaciÃ³n: devuelve 1 si la entrada es mayor o igual a 0, sino devuelve 0
    def activation_fn(self, x):
        return np.where(x >= 0, 1, 0)  # Implementa la funciÃ³n escalÃ³n

    # ğŸ”® MÃ©todo para hacer una predicciÃ³n
    def predict(self, x):
        # Producto punto entre pesos y entrada (x incluye bias)
        z = self.weights.T.dot(x)  # z = w0*x0 + w1*x1 + w2*x2
        return self.activation_fn(z)  # Aplica funciÃ³n escalÃ³n y devuelve 0 o 1

    # ğŸš€ MÃ©todo para entrenar el perceptrÃ³n
    def fit(self, X, d, epochs=10):
        # Itera sobre el nÃºmero de Ã©pocas (repeticiones sobre los datos)
        for _ in range(epochs):
            # Itera sobre cada muestra (xi) y su etiqueta (target)
            for xi, target in zip(X, d):
                output = self.predict(xi)  # PredicciÃ³n actual
                error = target - output  # Calcula error (deseado - obtenido)
                # Actualiza pesos usando la regla delta: w = w + lr * error * x
                self.weights += self.lr * error * xi

# ---------------------------------------------------
# ğŸ² Generamos datos aleatorios pero que sÃ­ son linealmente separables

# Generamos 50 puntos para la Clase 0, centrados cerca de (1,1)
class0 = np.random.randn(50, 2) * 0.3 + [1, 1]

# Generamos 50 puntos para la Clase 1, centrados cerca de (3,3)
class1 = np.random.randn(50, 2) * 0.3 + [3, 3]

# Unimos los puntos de ambas clases en un solo array
X_data = np.vstack((class0, class1))  # Ahora tenemos 100 puntos en total (50 + 50)

# ğŸš© Agregamos columna de 1s para el bias
# Ejemplo: si X_data = [x1, x2], X_bias = [1, x1, x2]
X_bias = np.hstack((np.ones((X_data.shape[0], 1)), X_data))

# Creamos las etiquetas: primeros 50 puntos son clase 0, los otros 50 son clase 1
d_labels = np.array([0]*50 + [1]*50)

# ---------------------------------------------------
# ğŸš€ Entrenamos el PerceptrÃ³n con nuestros datos
p = Perceptron(input_size=2)  # Creamos una instancia del perceptrÃ³n (2 entradas: x1 y x2)
p.fit(X_bias, d_labels, epochs=20)  # Entrenamos durante 20 Ã©pocas

# ---------------------------------------------------
# ğŸ¨ Graficamos los puntos en un plano

# Puntos rojos para la Clase 0
plt.scatter(class0[:,0], class0[:,1], color='red', label='Clase 0')
# Puntos azules para la Clase 1
plt.scatter(class1[:,0], class1[:,1], color='blue', label='Clase 1')

# ğŸ¯ Ahora graficamos la lÃ­nea separadora que aprendiÃ³ el perceptrÃ³n

# ğŸ“ EcuaciÃ³n de la lÃ­nea: w0*1 + w1*x + w2*y = 0
# Despejamos y: y = (-w0 - w1*x) / w2

# Generamos valores x (de 0 a 4) para trazar la lÃ­nea
x_vals = np.linspace(0, 4, 100)

# Calculamos los valores y usando la ecuaciÃ³n aprendida
y_vals = (-p.weights[0] - p.weights[1]*x_vals) / p.weights[2]

# Dibujamos la lÃ­nea verde
plt.plot(x_vals, y_vals, color='green', linewidth=2, label='LÃ­nea Separadora')

# ğŸ·ï¸ AÃ±adimos tÃ­tulo y etiquetas
plt.title('Separabilidad Lineal aprendida por PerceptrÃ³n')
plt.xlabel('x1')
plt.ylabel('x2')

# ğŸ“œ Mostramos leyenda
plt.legend()

# ğŸ—ºï¸ Mostramos la cuadrÃ­cula
plt.grid(True)

# ğŸš€ Finalmente mostramos la grÃ¡fica
plt.show()
